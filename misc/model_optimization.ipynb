{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Model for Deployment\n",
    "- Loading the model\n",
    "- Tensorflow runtime for model evaluation\n",
    "- Model pruning\n",
    "- Model quantization\n",
    "- Model compression\n",
    "- Model conversion to tflite\n",
    "- Model conversion to binary format .bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "import tempfile\n",
    "import keras\n",
    "from tensorflow.keras.models import Model\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the keras model\n",
    "model = load_model('model.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Austin\\AppData\\Local\\Temp\\tmp5vm6ipzc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Austin\\AppData\\Local\\Temp\\tmp5vm6ipzc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'serving_default': {'inputs': ['input_2'], 'outputs': ['dense_1']}}\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "\n",
    "# Print the signatures from the converted model\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "\n",
    "signatures = interpreter.get_signature_list()\n",
    "print(signatures)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Optimization\n",
    "#### Dynamic range quantization\n",
    "Dynamic range quantization is a recommended starting point because it provides reduced memory usage and faster computation without you having to provide a representative dataset for calibration.<br> \n",
    "This type of quantization, statically quantizes only the weights from floating point to integer at conversion time, which provides 8-bits of precision:<br>\n",
    "\n",
    "##### Model size: 3.2MB 87.2% smaller<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Optimization option OPTIMIZE_FOR_SIZE is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Austin\\AppData\\Local\\Temp\\tmp63bmpa50\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Austin\\AppData\\Local\\Temp\\tmp63bmpa50\\assets\n",
      "WARNING:absl:Optimization option OPTIMIZE_FOR_SIZE is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n",
      "WARNING:absl:Optimization option OPTIMIZE_FOR_SIZE is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# quantized_and_pruned_tflite_model = converter.convert()\n",
    "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# converter.target_spec.supported_types = [tf.float16]\n",
    "quantized_and_pruned_tflite_model = converter.convert()\n",
    "\n",
    "with open('quantized_and_pruned_tflite2.tflite', 'wb') as f:\n",
    "  f.write(quantized_and_pruned_tflite_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full integer quantization\n",
    "You can get further latency improvements, reductions in peak memory usage, and compatibility with integer only hardware devices or accelerators by making sure all model math is integer quantized.\n",
    "\n",
    "For full integer quantization, you need to calibrate or estimate the range, i.e, (min, max) of all floating-point tensors in the model.<br> Unlike constant tensors such as weights and biases, variable tensors such as model input, activations (outputs of intermediate layers) and model output cannot be calibrated unless we run a few inference cycles.<br> As a result, the converter requires a representative dataset to calibrate them. This dataset can be a small subset (around ~100-500 samples) of the training or validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Austin\\Desktop\\Agent\\dsail-tech4wildlife\\dsa2022-Arusha/camera-trap/porini-machine-learning/data/porinicroppedimages/dataset/train/\n",
      "c:\\Users\\Austin\\Desktop\\Agent\\dsail-tech4wildlife\\dsa2022-Arusha/camera-trap/porini-machine-learning/data/porinicroppedimages/dataset/test/\n",
      "(2086, 9)\n",
      "Found 1850 validated image filenames belonging to 6 classes.\n",
      "Found 236 validated image filenames belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "#set directories.\n",
    "base_dir = os.path.join(os.getcwd(), 'dsa2022-Arusha/camera-trap/porini-machine-learning/data/porinicroppedimages/dataset/')\n",
    "# base_dir = './data/porinicroppedimages/dataset/'\n",
    "\n",
    "\n",
    "# Directory with our training images\n",
    "train_dir = os.path.join(base_dir, 'train/')\n",
    "\n",
    "print(train_dir)\n",
    "\n",
    "#Directory with test images.\n",
    "test_dir = os.path.join(base_dir, 'test/')\n",
    "print(test_dir)\n",
    "\n",
    "df_train = pd.read_csv(base_dir + 'train.csv')\n",
    "print(df_train.shape)\n",
    "\n",
    "df_test = pd.read_csv(base_dir + 'test.csv')\n",
    "\n",
    "seed = 2022\n",
    "train, val = train_test_split(df_train, test_size = 0.113, random_state = seed)\n",
    "\n",
    "size = (128,128)\n",
    "batch_size = 32\n",
    "\n",
    "train_size = train.shape[0]\n",
    "val_size = val.shape[0]\n",
    "\n",
    "#to train on whole data per iteration\n",
    "train_steps_per_epoch = int(train_size/batch_size) #int here used to round off\n",
    "val_steps_per_epoch = int(val_size/batch_size) #int here used to round off\n",
    "\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(   \n",
    "                # rotation_range = 30,\n",
    "                # width_shift_range = 0.2,#\n",
    "                # height_shift_range = 0.2,#\n",
    "                # brightness_range = [0.5,1.5],#\n",
    "                # horizontal_flip = True,\n",
    "                # fill_mode = 'nearest'\n",
    ")\n",
    "\n",
    "\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "                    train,\n",
    "                    directory = train_dir,\n",
    "                    x_col = \"filename\",\n",
    "                    y_col = \"Species\",\n",
    "                    target_size = size,\n",
    "                    class_mode = \"categorical\",\n",
    "                    batch_size = batch_size,\n",
    "                    shuffle = True,\n",
    "                    seed = seed,\n",
    "                    interpolation = \"nearest\",\n",
    "                    #validate_filenames=False\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "                    val,\n",
    "                    directory = train_dir, #valid is from train\n",
    "                    x_col = \"filename\",\n",
    "                    y_col = \"Species\",\n",
    "                    target_size = size,\n",
    "                    class_mode = \"categorical\",\n",
    "                    batch_size = batch_size,\n",
    "                    shuffle = True,\n",
    "                    seed = seed,\n",
    "                    interpolation = \"nearest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensors = []\n",
    "\n",
    "for _ in range(len(val_generator)):\n",
    "    batch_images = next(val_generator)\n",
    "    image_tensors.extend(batch_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128, 3)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using pytorch to get image \n",
    "next(val_generator)[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_dataset():\n",
    "    for data in tf.data.Dataset.from_tensor_slices(val_generator).batch(1).take(100):\n",
    "        yield tf.cast(data, tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Austin\\AppData\\Local\\Temp\\tmpn1fzmc3z\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Austin\\AppData\\Local\\Temp\\tmpn1fzmc3z\\assets\n",
      "C:\\Users\\Austin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\lite\\python\\convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# converter.target_spec.supported_types = [tf.float16]\n",
    "converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "tflite_quant_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2322 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.keras.utils.image_dataset_from_directory(base_dir, batch_size=1, image_size=(128, 128))\n",
    "\n",
    "def representative_dataset():\n",
    "    for images, _ in dataset:\n",
    "        yield [images]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tflite_quant_model2.tflite', 'wb') as f:\n",
    "  f.write(tflite_quant_model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a representative dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2322 files belonging to 2 classes.\n",
      "Representative features saved as representative_features.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the dataset\n",
    "dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    base_dir,\n",
    "    batch_size=1,\n",
    "    image_size=(128, 128)\n",
    ")\n",
    "\n",
    "# Create an empty list to store the representative features\n",
    "representative_features = []\n",
    "\n",
    "# Iterate over the dataset and extract the images\n",
    "for images, _ in dataset:\n",
    "    # Append the images to the list\n",
    "    representative_features.append(images.numpy())\n",
    "\n",
    "# Convert the list to a NumPy array\n",
    "representative_features = np.concatenate(representative_features, axis=0)\n",
    "\n",
    "# Save the representative features as an .npy file\n",
    "np.save('representative_features.npy', representative_features)\n",
    "\n",
    "print(\"Representative features saved as representative_features.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2322 files belonging to 2 classes.\n",
      "Batch shape: (1, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.keras.utils.image_dataset_from_directory(base_dir, batch_size=1, image_size=(128, 128))\n",
    "\n",
    "for images, labels in dataset:\n",
    "    # Accessing the shape of the batched images\n",
    "    batch_shape = images.shape\n",
    "    print(\"Batch shape:\", batch_shape)\n",
    "    break  # Only printing the shape of the first batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[128.70117  , 108.70117  , 109.70117  ],\n",
       "         [125.29883  , 105.29883  , 106.29883  ],\n",
       "         [124.00781  , 104.00781  , 105.00781  ],\n",
       "         ...,\n",
       "         [180.5039   , 154.5039   , 155.5039   ],\n",
       "         [182.90234  , 156.90234  , 157.90234  ],\n",
       "         [182.09961  , 158.09961  , 158.09961  ]],\n",
       "\n",
       "        [[128.09961  , 108.09961  , 109.09961  ],\n",
       "         [128.90234  , 108.90234  , 109.90234  ],\n",
       "         [128.99414  , 108.99414  , 109.99414  ],\n",
       "         ...,\n",
       "         [176.50195  , 150.50195  , 151.50195  ],\n",
       "         [178.90234  , 152.90234  , 153.90234  ],\n",
       "         [178.90039  , 154.90039  , 154.90039  ]],\n",
       "\n",
       "        [[128.19922  , 108.19922  , 109.19922  ],\n",
       "         [130.       , 110.       , 111.       ],\n",
       "         [130.4961   , 110.49609  , 111.49609  ],\n",
       "         ...,\n",
       "         [176.99805  , 150.99805  , 151.99805  ],\n",
       "         [176.59766  , 150.59766  , 151.59766  ],\n",
       "         [174.90039  , 150.90039  , 150.90039  ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[109.00195  ,  89.00195  ,  78.00195  ],\n",
       "         [107.09766  ,  87.09766  ,  76.09766  ],\n",
       "         [104.00586  ,  87.99805  ,  75.00195  ],\n",
       "         ...,\n",
       "         [140.51953  , 105.51953  ,  99.51953  ],\n",
       "         [150.10938  , 117.109375 , 110.109375 ],\n",
       "         [156.79688  , 125.99805  , 112.99609  ]],\n",
       "\n",
       "        [[111.40039  ,  91.40039  ,  80.40039  ],\n",
       "         [104.79883  ,  84.79883  ,  73.79883  ],\n",
       "         [108.521484 ,  92.51367  ,  79.51758  ],\n",
       "         ...,\n",
       "         [130.01172  ,  95.01172  ,  89.01172  ],\n",
       "         [138.4082   , 105.4082   ,  98.4082   ],\n",
       "         [145.40039  , 116.60156  , 103.19922  ]],\n",
       "\n",
       "        [[103.19922  ,  83.19922  ,  72.19922  ],\n",
       "         [104.90234  ,  84.90234  ,  73.90234  ],\n",
       "         [ 96.01953  ,  80.01172  ,  67.015625 ],\n",
       "         ...,\n",
       "         [141.98242  , 106.98242  , 100.98242  ],\n",
       "         [141.5      , 108.5      , 101.5      ],\n",
       "         [137.70508  , 104.70508  ,  92.603516 ]]],\n",
       "\n",
       "\n",
       "       [[[ 71.85547  ,  13.855469 ,  37.85547  ],\n",
       "         [ 71.43359  ,  13.433594 ,  37.433594 ],\n",
       "         [ 74.16797  ,  16.167969 ,  40.16797  ],\n",
       "         ...,\n",
       "         [105.38672  ,  34.38672  ,  64.38672  ],\n",
       "         [107.86719  ,  36.867188 ,  66.86719  ],\n",
       "         [101.01172  ,  30.011719 ,  60.01172  ]],\n",
       "\n",
       "        [[ 74.11771  ,  16.117706 ,  40.117706 ],\n",
       "         [ 75.38672  ,  17.386719 ,  41.38672  ],\n",
       "         [ 77.406906 ,  19.406906 ,  43.406906 ],\n",
       "         ...,\n",
       "         [102.14778  ,  31.147781 ,  61.14778  ],\n",
       "         [103.354294 ,  32.354294 ,  62.354294 ],\n",
       "         [ 95.22487  ,  24.224869 ,  54.22487  ]],\n",
       "\n",
       "        [[ 72.46701  ,  17.408417 ,  40.427948 ],\n",
       "         [ 75.85872  ,  20.800125 ,  43.819656 ],\n",
       "         [ 77.71724  ,  22.658646 ,  45.678177 ],\n",
       "         ...,\n",
       "         [ 99.16797  ,  28.167969 ,  58.16797  ],\n",
       "         [100.359375 ,  29.359375 ,  59.359375 ],\n",
       "         [ 93.892365 ,  22.892365 ,  52.892365 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 51.224167 ,   4.0796356,  20.079636 ],\n",
       "         [ 70.012344 ,  20.711563 ,  37.145157 ],\n",
       "         [ 84.03578  ,  30.145157 ,  48.59047  ],\n",
       "         ...,\n",
       "         [ 47.954636 ,   3.6827087,  20.410782 ],\n",
       "         [ 50.16472  ,   5.7395935,  22.314468 ],\n",
       "         [ 65.112854 ,  18.596573 ,  36.435333 ]],\n",
       "\n",
       "        [[ 61.501556 ,  14.357025 ,  30.357025 ],\n",
       "         [ 73.86951  ,  24.568726 ,  41.00232  ],\n",
       "         [ 82.88669  ,  29.013    ,  47.449844 ],\n",
       "         ...,\n",
       "         [ 51.319626 ,   7.3313446,  24.331345 ],\n",
       "         [ 53.67581  ,   9.6875305,  26.68753  ],\n",
       "         [ 64.70274  ,  20.714462 ,  37.714462 ]],\n",
       "\n",
       "        [[ 78.43359  ,  31.289062 ,  47.289062 ],\n",
       "         [ 82.30078  ,  33.       ,  49.433594 ],\n",
       "         [ 82.55469  ,  30.109375 ,  47.83203  ],\n",
       "         ...,\n",
       "         [ 64.109375 ,  21.109375 ,  38.109375 ],\n",
       "         [ 65.86719  ,  22.867188 ,  39.867188 ],\n",
       "         [ 65.       ,  22.       ,  39.       ]]],\n",
       "\n",
       "\n",
       "       [[[ 52.       ,  44.       ,  33.       ],\n",
       "         [ 51.691406 ,  43.691406 ,  32.691406 ],\n",
       "         [ 51.152344 ,  43.152344 ,  32.152344 ],\n",
       "         ...,\n",
       "         [ 32.304688 ,  25.304688 ,  15.3046875],\n",
       "         [ 33.382812 ,  26.382812 ,  16.382812 ],\n",
       "         [ 34.       ,  27.       ,  17.       ]],\n",
       "\n",
       "        [[ 52.007812 ,  44.007812 ,  33.007812 ],\n",
       "         [ 51.700424 ,  43.700424 ,  32.700424 ],\n",
       "         [ 51.163467 ,  43.163467 ,  32.163467 ],\n",
       "         ...,\n",
       "         [ 32.304688 ,  25.304688 ,  15.3046875],\n",
       "         [ 33.382812 ,  26.382812 ,  16.382812 ],\n",
       "         [ 34.       ,  27.       ,  17.       ]],\n",
       "\n",
       "        [[ 52.679688 ,  44.679688 ,  33.679688 ],\n",
       "         [ 52.475967 ,  44.475967 ,  33.475967 ],\n",
       "         [ 52.120102 ,  44.120102 ,  33.120102 ],\n",
       "         ...,\n",
       "         [ 32.304688 ,  25.304688 ,  15.3046875],\n",
       "         [ 33.382812 ,  26.382812 ,  16.382812 ],\n",
       "         [ 34.       ,  27.       ,  17.       ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 73.67969  ,  66.67969  ,  40.679688 ],\n",
       "         [ 74.39572  ,  67.39572  ,  41.39572  ],\n",
       "         [ 75.646515 ,  68.646515 ,  42.646515 ],\n",
       "         ...,\n",
       "         [ 78.0674   ,  72.0674   ,  40.067398 ],\n",
       "         [ 77.17247  ,  71.17247  ,  39.17247  ],\n",
       "         [ 76.66016  ,  70.66016  ,  38.660156 ]],\n",
       "\n",
       "        [[ 73.00781  ,  66.00781  ,  40.007812 ],\n",
       "         [ 73.93118  ,  66.93118  ,  40.931183 ],\n",
       "         [ 75.54416  ,  68.54416  ,  42.54416  ],\n",
       "         ...,\n",
       "         [ 78.688095 ,  72.688095 ,  40.688095 ],\n",
       "         [ 77.612076 ,  71.612076 ,  39.612076 ],\n",
       "         [ 76.99609  ,  70.99609  ,  38.996094 ]],\n",
       "\n",
       "        [[ 73.       ,  66.       ,  40.       ],\n",
       "         [ 73.92578  ,  66.92578  ,  40.92578  ],\n",
       "         [ 75.54297  ,  68.54297  ,  42.54297  ],\n",
       "         ...,\n",
       "         [ 78.69531  ,  72.69531  ,  40.695312 ],\n",
       "         [ 77.61719  ,  71.61719  ,  39.617188 ],\n",
       "         [ 77.       ,  71.       ,  39.       ]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 29.       ,  30.       ,  25.       ],\n",
       "         [ 29.039062 ,  30.039062 ,  25.039062 ],\n",
       "         [ 29.398438 ,  30.398438 ,  25.398438 ],\n",
       "         ...,\n",
       "         [ 38.601562 ,  42.601562 ,  45.601562 ],\n",
       "         [ 38.960938 ,  42.960938 ,  45.960938 ],\n",
       "         [ 39.       ,  43.       ,  46.       ]],\n",
       "\n",
       "        [[ 28.992188 ,  29.992188 ,  24.992188 ],\n",
       "         [ 29.03125  ,  30.03125  ,  25.03125  ],\n",
       "         [ 29.390625 ,  30.390625 ,  25.390625 ],\n",
       "         ...,\n",
       "         [ 38.592957 ,  42.592957 ,  45.592957 ],\n",
       "         [ 38.949524 ,  42.949524 ,  45.949524 ],\n",
       "         [ 38.98828  ,  42.98828  ,  45.98828  ]],\n",
       "\n",
       "        [[ 28.320312 ,  29.320312 ,  24.320312 ],\n",
       "         [ 28.359375 ,  29.359375 ,  24.359375 ],\n",
       "         [ 28.71875  ,  29.71875  ,  24.71875  ],\n",
       "         ...,\n",
       "         [ 37.852844 ,  41.852844 ,  44.852844 ],\n",
       "         [ 37.967957 ,  41.967957 ,  44.967957 ],\n",
       "         [ 37.98047  ,  41.98047  ,  44.98047  ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 58.01953  ,  60.01953  ,  46.01953  ],\n",
       "         [ 58.08438  ,  60.08438  ,  46.08438  ],\n",
       "         [ 58.681    ,  60.681    ,  46.681    ],\n",
       "         ...,\n",
       "         [ 65.28125  ,  67.28125  ,  53.28125  ],\n",
       "         [ 65.640625 ,  67.640625 ,  53.640625 ],\n",
       "         [ 65.67969  ,  67.67969  ,  53.679688 ]],\n",
       "\n",
       "        [[ 57.01172  ,  59.01172  ,  45.01172  ],\n",
       "         [ 57.08969  ,  59.08969  ,  45.08969  ],\n",
       "         [ 57.807037 ,  59.807037 ,  45.807037 ],\n",
       "         ...,\n",
       "         [ 64.609375 ,  66.609375 ,  52.609375 ],\n",
       "         [ 64.96875  ,  66.96875  ,  52.96875  ],\n",
       "         [ 65.00781  ,  67.00781  ,  53.007812 ]],\n",
       "\n",
       "        [[ 57.       ,  59.       ,  45.       ],\n",
       "         [ 57.078125 ,  59.078125 ,  45.078125 ],\n",
       "         [ 57.796875 ,  59.796875 ,  45.796875 ],\n",
       "         ...,\n",
       "         [ 64.60156  ,  66.60156  ,  52.601562 ],\n",
       "         [ 64.96094  ,  66.96094  ,  52.960938 ],\n",
       "         [ 65.       ,  67.       ,  53.       ]]],\n",
       "\n",
       "\n",
       "       [[[ 19.       ,  19.       ,  19.       ],\n",
       "         [ 18.480469 ,  18.480469 ,  18.480469 ],\n",
       "         [ 18.       ,  18.       ,  18.       ],\n",
       "         ...,\n",
       "         [ 12.       ,  16.       ,  15.       ],\n",
       "         [ 12.480469 ,  16.480469 ,  15.480469 ],\n",
       "         [ 13.       ,  17.       ,  16.       ]],\n",
       "\n",
       "        [[ 18.585938 ,  18.585938 ,  18.585938 ],\n",
       "         [ 18.281525 ,  18.281525 ,  18.281525 ],\n",
       "         [ 18.       ,  18.       ,  18.       ],\n",
       "         ...,\n",
       "         [ 12.4140625,  16.414062 ,  15.4140625],\n",
       "         [ 13.093475 ,  17.093475 ,  16.093475 ],\n",
       "         [ 13.828125 ,  17.828125 ,  16.828125 ]],\n",
       "\n",
       "        [[ 18.       ,  18.       ,  18.       ],\n",
       "         [ 18.       ,  18.       ,  18.       ],\n",
       "         [ 18.       ,  18.       ,  18.       ],\n",
       "         ...,\n",
       "         [ 13.018768 ,  17.018768 ,  16.018768 ],\n",
       "         [ 13.984375 ,  17.984375 ,  16.984375 ],\n",
       "         [ 15.0234375,  19.023438 ,  18.023438 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 27.023438 ,  34.023438 ,  18.023438 ],\n",
       "         [ 27.023438 ,  34.023438 ,  18.023438 ],\n",
       "         [ 27.023438 ,  34.023438 ,  18.023438 ],\n",
       "         ...,\n",
       "         [ 34.957794 ,  37.00467  ,  23.981232 ],\n",
       "         [ 35.444855 ,  37.49173  ,  24.468292 ],\n",
       "         [ 35.976562 ,  38.023438 ,  25.       ]],\n",
       "\n",
       "        [[ 27.       ,  34.       ,  18.       ],\n",
       "         [ 26.695587 ,  33.695587 ,  17.695587 ],\n",
       "         [ 26.530792 ,  33.530792 ,  17.530792 ],\n",
       "         ...,\n",
       "         [ 34.530792 ,  36.530792 ,  23.530792 ],\n",
       "         [ 34.89453  ,  36.89453  ,  23.894531 ],\n",
       "         [ 35.414062 ,  37.414062 ,  24.414062 ]],\n",
       "\n",
       "        [[ 27.       ,  34.       ,  18.       ],\n",
       "         [ 26.480469 ,  33.48047  ,  17.480469 ],\n",
       "         [ 26.199219 ,  33.19922  ,  17.199219 ],\n",
       "         ...,\n",
       "         [ 34.19922  ,  36.19922  ,  23.199219 ],\n",
       "         [ 34.48047  ,  36.48047  ,  23.480469 ],\n",
       "         [ 35.       ,  37.       ,  24.       ]]],\n",
       "\n",
       "\n",
       "       [[[133.       , 111.       , 113.       ],\n",
       "         [131.74292  , 109.74292  , 111.74292  ],\n",
       "         [133.84644  , 111.846436 , 113.846436 ],\n",
       "         ...,\n",
       "         [182.4231   , 156.4231   , 157.4231   ],\n",
       "         [183.78906  , 157.78906  , 158.78906  ],\n",
       "         [184.47656  , 158.47656  , 159.47656  ]],\n",
       "\n",
       "        [[129.8601   , 107.86011  , 109.86011  ],\n",
       "         [131.09595  , 109.09595  , 111.09595  ],\n",
       "         [129.3899   , 107.38989  , 109.38989  ],\n",
       "         ...,\n",
       "         [182.8396   , 156.8396   , 157.8396   ],\n",
       "         [183.4375   , 157.4375   , 158.4375   ],\n",
       "         [186.69849  , 160.69849  , 161.69849  ]],\n",
       "\n",
       "        [[129.52734  , 107.52734  , 109.52734  ],\n",
       "         [128.71484  , 106.71484  , 108.71484  ],\n",
       "         [130.31201  , 108.31201  , 110.31201  ],\n",
       "         ...,\n",
       "         [183.35669  , 157.35669  , 158.35669  ],\n",
       "         [185.52686  , 159.52686  , 160.52686  ],\n",
       "         [182.52612  , 156.52612  , 157.52612  ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[107.70703  ,  91.28784  ,  79.54541  ],\n",
       "         [101.29297  ,  85.29297  ,  72.29297  ],\n",
       "         [108.41919  ,  92.41919  ,  77.54419  ],\n",
       "         ...,\n",
       "         [155.79541  , 121.92041  , 111.85791  ],\n",
       "         [153.69434  , 121.694336 , 110.694336 ],\n",
       "         [150.48486  , 119.32324  , 108.32324  ]],\n",
       "\n",
       "        [[103.28516  ,  86.28516  ,  76.28516  ],\n",
       "         [104.48828  ,  88.48828  ,  75.48828  ],\n",
       "         [109.261475 ,  93.261475 ,  78.386475 ],\n",
       "         ...,\n",
       "         [145.26685  , 111.391846 , 101.329346 ],\n",
       "         [147.05273  , 115.052734 , 104.052734 ],\n",
       "         [151.00903  , 121.00903  , 110.00903  ]],\n",
       "\n",
       "        [[ 98.812744 ,  81.812744 ,  71.812744 ],\n",
       "         [101.77344  ,  85.77344  ,  72.77344  ],\n",
       "         [104.451904 ,  88.451904 ,  73.576904 ],\n",
       "         ...,\n",
       "         [139.9873   , 106.112305 ,  96.049805 ],\n",
       "         [139.0625   , 107.0625   ,  96.0625   ],\n",
       "         [149.54297  , 119.54297  , 108.54297  ]]]], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "representative_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_dataset():\n",
    "      for data in dataset:\n",
    "            yield {\n",
    "            \"image\": data.image,\n",
    "            \"bias\": data.bias,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You called `set_weights(weights)` on layer \"flatten_3\" with a weight list of length 2, but the layer was expecting 0 weights. Provided weights: [array([[[[ 0.00130486, -0.03660612,  0.10142724, ...",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Austin\\Desktop\\Agent\\dsail-tech4wildlife\\model_optimization.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Austin/Desktop/Agent/dsail-tech4wildlife/model_optimization.ipynb#X30sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# Transfer the weights from the original model to the smaller model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Austin/Desktop/Agent/dsail-tech4wildlife/model_optimization.ipynb#X30sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer_target, layer_source \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(small_model\u001b[39m.\u001b[39mlayers, original_model\u001b[39m.\u001b[39mlayers):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Austin/Desktop/Agent/dsail-tech4wildlife/model_optimization.ipynb#X30sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     layer_target\u001b[39m.\u001b[39;49mset_weights(layer_source\u001b[39m.\u001b[39;49mget_weights())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Austin/Desktop/Agent/dsail-tech4wildlife/model_optimization.ipynb#X30sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m# Quantize the weights and activations\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Austin/Desktop/Agent/dsail-tech4wildlife/model_optimization.ipynb#X30sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m converter \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mlite\u001b[39m.\u001b[39mTFLiteConverter\u001b[39m.\u001b[39mfrom_keras_model(small_model)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\base_layer.py:1747\u001b[0m, in \u001b[0;36mLayer.set_weights\u001b[1;34m(self, weights)\u001b[0m\n\u001b[0;32m   1744\u001b[0m         expected_num_weights \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1746\u001b[0m \u001b[39mif\u001b[39;00m expected_num_weights \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(weights):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1748\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mYou called `set_weights(weights)` on layer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1749\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mwith a weight list of length \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, but the layer was \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1750\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mexpecting \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m weights. Provided weights: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1751\u001b[0m         \u001b[39m%\u001b[39m (\n\u001b[0;32m   1752\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname,\n\u001b[0;32m   1753\u001b[0m             \u001b[39mlen\u001b[39m(weights),\n\u001b[0;32m   1754\u001b[0m             expected_num_weights,\n\u001b[0;32m   1755\u001b[0m             \u001b[39mstr\u001b[39m(weights)[:\u001b[39m50\u001b[39m],\n\u001b[0;32m   1756\u001b[0m         )\n\u001b[0;32m   1757\u001b[0m     )\n\u001b[0;32m   1759\u001b[0m weight_index \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m   1760\u001b[0m weight_value_tuples \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mValueError\u001b[0m: You called `set_weights(weights)` on layer \"flatten_3\" with a weight list of length 2, but the layer was expecting 0 weights. Provided weights: [array([[[[ 0.00130486, -0.03660612,  0.10142724, ..."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import get_file\n",
    "import tempfile\n",
    "import zipfile\n",
    "\n",
    "# Load the original Keras model\n",
    "original_model = load_model('model.h5')\n",
    "\n",
    "# Define a smaller model architecture\n",
    "small_model = tf.keras.Sequential([\n",
    "    layers.InputLayer(input_shape=(128, 128, 3)),  # Specify the input shape of your model\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=shape)\n",
    "    # Convolutional layers\n",
    "    # layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    # layers.MaxPooling2D((2, 2)),\n",
    "    # layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    # layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Flatten layer\n",
    "    layers.Flatten(),\n",
    "    \n",
    "    # Dense layers\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(6, activation='softmax')  # Assuming 10 classes for classification\n",
    "    \n",
    "    # Add more layers as needed\n",
    "])\n",
    "\n",
    "# Transfer the weights from the original model to the smaller model\n",
    "for layer_target, layer_source in zip(small_model.layers, original_model.layers):\n",
    "    layer_target.set_weights(layer_source.get_weights())\n",
    "\n",
    "# Quantize the weights and activations\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(small_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "# Save the quantized model to a file\n",
    "tflite_model_path = 'quantized_model.tflite'\n",
    "with open(tflite_model_path, 'wb') as f:\n",
    "    f.write(tflite_quant_model)\n",
    "\n",
    "# Compress the model file\n",
    "compressed_model_path = 'compressed_model.zip'\n",
    "with zipfile.ZipFile(compressed_model_path, 'w', compression=zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write(tflite_model_path, arcname='model.tflite')\n",
    "\n",
    "# Check the compressed model size\n",
    "compressed_model_size = os.path.getsize(compressed_model_path)\n",
    "print(f\"Compressed model size: {compressed_model_size / (1024 * 1024):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer conv2d_29 weight shape (3, 3, 3, 16) is not compatible with provided weight shape (3, 3, 3, 32).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Austin\\Desktop\\Agent\\dsail-tech4wildlife\\model_optimization.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Austin/Desktop/Agent/dsail-tech4wildlife/model_optimization.ipynb#X36sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer_target, layer_source \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(smaller_model\u001b[39m.\u001b[39mlayers[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], bigger_model\u001b[39m.\u001b[39mlayers[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Austin/Desktop/Agent/dsail-tech4wildlife/model_optimization.ipynb#X36sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(layer_target, tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mConv2D):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Austin/Desktop/Agent/dsail-tech4wildlife/model_optimization.ipynb#X36sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m         \u001b[39m# Adjust the number of filters in Conv2D layers\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Austin/Desktop/Agent/dsail-tech4wildlife/model_optimization.ipynb#X36sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m         layer_target\u001b[39m.\u001b[39;49mset_weights([layer_source\u001b[39m.\u001b[39;49mget_weights()[\u001b[39m0\u001b[39;49m][:, :, :\u001b[39m16\u001b[39;49m, :], layer_source\u001b[39m.\u001b[39;49mget_weights()[\u001b[39m1\u001b[39;49m]])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Austin/Desktop/Agent/dsail-tech4wildlife/model_optimization.ipynb#X36sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Austin/Desktop/Agent/dsail-tech4wildlife/model_optimization.ipynb#X36sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m         layer_target\u001b[39m.\u001b[39mset_weights(layer_source\u001b[39m.\u001b[39mget_weights())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\base_layer.py:1772\u001b[0m, in \u001b[0;36mLayer.set_weights\u001b[1;34m(self, weights)\u001b[0m\n\u001b[0;32m   1770\u001b[0m ref_shape \u001b[39m=\u001b[39m param\u001b[39m.\u001b[39mshape\n\u001b[0;32m   1771\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ref_shape\u001b[39m.\u001b[39mis_compatible_with(weight_shape):\n\u001b[1;32m-> 1772\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1773\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLayer \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m weight shape \u001b[39m\u001b[39m{\u001b[39;00mref_shape\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1774\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mis not compatible with provided weight \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1775\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshape \u001b[39m\u001b[39m{\u001b[39;00mweight_shape\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1776\u001b[0m     )\n\u001b[0;32m   1777\u001b[0m weight_value_tuples\u001b[39m.\u001b[39mappend((param, weight))\n\u001b[0;32m   1778\u001b[0m weight_index \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: Layer conv2d_29 weight shape (3, 3, 3, 16) is not compatible with provided weight shape (3, 3, 3, 32)."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Define the input shape of the image\n",
    "input_shape = (128, 128, 3)\n",
    "\n",
    "# Define the bigger model\n",
    "bigger_model = models.Sequential()\n",
    "bigger_model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "bigger_model.add(layers.MaxPooling2D((2, 2)))\n",
    "bigger_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "bigger_model.add(layers.MaxPooling2D((2, 2)))\n",
    "bigger_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "bigger_model.add(layers.Flatten())\n",
    "bigger_model.add(layers.Dense(64, activation='relu'))\n",
    "bigger_model.add(layers.Dense(6, activation='softmax'))\n",
    "\n",
    "# Load the weights into the bigger model\n",
    "bigger_model.load_weights('model.h5')\n",
    "\n",
    "# Define a smaller model architecture\n",
    "smaller_model = models.Sequential()\n",
    "smaller_model.add(layers.InputLayer(input_shape=input_shape))\n",
    "smaller_model.add(layers.Conv2D(16, (3, 3), activation='relu'))\n",
    "smaller_model.add(layers.MaxPooling2D((2, 2)))\n",
    "smaller_model.add(layers.Conv2D(16, (3, 3), activation='relu'))\n",
    "smaller_model.add(layers.MaxPooling2D((2, 2)))\n",
    "smaller_model.add(layers.Flatten())\n",
    "smaller_model.add(layers.Dense(64, activation='relu'))\n",
    "smaller_model.add(layers.Dense(6, activation='softmax'))\n",
    "\n",
    "# Transfer the weights from the bigger model to the smaller model\n",
    "for layer_target, layer_source in zip(smaller_model.layers[:-1], bigger_model.layers[:-1]):\n",
    "    if isinstance(layer_target, tf.keras.layers.Conv2D):\n",
    "        # Adjust the number of filters in Conv2D layers\n",
    "        layer_target.set_weights([layer_source.get_weights()[0][:, :, :16, :], layer_source.get_weights()[1]])\n",
    "    else:\n",
    "        layer_target.set_weights(layer_source.get_weights())\n",
    "\n",
    "# Quantize the weights and activations\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(smaller_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "# Save the quantized model to a file\n",
    "tflite_model_path = 'quantized_model.tflite'\n",
    "with open(tflite_model_path, 'wb') as f:\n",
    "    f.write(tflite_quant_model)\n",
    "\n",
    "# Compress the model file\n",
    "compressed_model_path = 'compressed_model.zip'\n",
    "with zipfile.ZipFile(compressed_model_path, 'w', compression=zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write(tflite_model_path, arcname='model.tflite')\n",
    "\n",
    "# Check the compressed model size\n",
    "compressed_model_size = os.path.getsize(compressed_model_path)\n",
    "print(f\"Compressed model size: {compressed_model_size / (1024 * 1024):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf2onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tf2onnx' has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Austin\\Desktop\\Agent\\dsail-tech4wildlife\\model_optimization.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Austin/Desktop/Agent/dsail-tech4wildlife/model_optimization.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Save the ONNX model to a file\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Austin/Desktop/Agent/dsail-tech4wildlife/model_optimization.ipynb#X41sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m onnx_model_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mconverted_model.onnx\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Austin/Desktop/Agent/dsail-tech4wildlife/model_optimization.ipynb#X41sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m tf2onnx\u001b[39m.\u001b[39;49msave\u001b[39m.\u001b[39msave_model(onnx_model, onnx_model_path)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Austin/Desktop/Agent/dsail-tech4wildlife/model_optimization.ipynb#X41sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mKeras model converted to ONNX successfully.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tf2onnx' has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "# Convert the Keras model to ONNX\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(model)\n",
    "\n",
    "# Save the ONNX model to a file\n",
    "onnx_model_path = 'converted_model.onnx'\n",
    "tf2onnx.save.save_model(onnx_model, onnx_model_path)\n",
    "\n",
    "print(\"Keras model converted to ONNX successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model quantized and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic\n",
    "\n",
    "# Load the ONNX model\n",
    "# onnx_model = onnx.load('path_to_onnx_model.onnx')\n",
    "\n",
    "# Quantize the model\n",
    "# quantized_model = quantize_dynamic(onnx_model)\n",
    "\n",
    "# Save the quantized model\n",
    "# quantized_model_path = 'quantized_model.onnx'\n",
    "onnx.save_model(onnx_model, onnx_model_path)\n",
    "\n",
    "print(\"ONNX model quantized and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
